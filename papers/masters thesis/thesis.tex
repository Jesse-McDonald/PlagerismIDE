\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{bbold}

\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{xcolor,colortbl}
\usepackage{dblfloatfix}
\newcommand{\cbox}[1]{\raisebox{\depth}{\fcolorbox{black}{#1}{\null}}}
\newcommand{\n}{\hfill\break}

\newcommand{\installID}{InstallID}
\newcommand{\projectID}{ProjectID}
\newcommand{\infectionStack}{InfectionStack}
%\usepackage{svg}

\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={green!75!black},
	urlcolor={blue!80!black}
}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\makeatletter
\newlength{\logoheight}
\setlength{\logoheight}{50pt} % set your logo height

    

\title{
    \textbf{Title}}

\author{\IEEEauthorblockN{McDonald, Jesse}
	\IEEEauthorblockA{
	University of Hawai`i M\=anoa\\
	jamcd@hawaii.edu
	}
}

\maketitle

\begin{abstract}
abstract
\end{abstract}

\begin{IEEEkeywords}
keyword
\end{IEEEkeywords}

\section{\textbf{Introduction}}
	Introductory Computer Science classes are often students first look at programming.  In these classes people who dont know how to program, learn to program for the first time.  Or that is the assumption.  However, due to the nature of computer science it is often possible to pass an intro to computer science course without writing a single line of code by plagiarizing assignments.
	\subsection{Plagiarisms Logical Consequences}
		Plagiarism is particularly troublesome in intro classes.  Since the intro classes are often a students first experience programming, if a student passes an intro class by plagiarizing it is likely that student does not understand part or all of the course.  Since higher programming courses assume you have the basics, these students have only 2 choices going forward.  Firstly they can work hard and put in extra time to teach themselves the missed information.  This is unlikely, as this student has already demonstrated they are unwilling to or unable to learn the basics by cheating on them to begin with.
		Or Second, they must keep plagiarizing.  This option ultimately leads to a student who does not know how to program potentially graduating with a Computer Science degree.  If this student attempts to use this degree to apply for a job, the interview process, or even worst the opening weeks of a job, will reveal this lack of knowledge.  Such a revelation is harmful to the entire community of computer science as it erodes confidence in the degree, and raises the bar for "entry level" job experience.
		\subsection{Prevalence}
		 One may ask, "How prevalent is plagiarism in intro CS classes." So did we, In an single 60 student intro level during class 2020, we observed one assignment where 10 people turned in exactly the same assignment, and nearly a third of the class had recognizable plagiarized assignments.   This assignment will be referred to later as the ``Motivating Case.''
		 [[xxxxx Find papers referencing plagiarism]]
		 Additionally, we have known many high level computer science students who don't seam to understand basic programming.  While this lack of understanding may not point to plagiarism, that seams the most likely culprit.
		
\n\section{\textbf{Definitions}}
Plagiarism is generally defined as ``The practice of taking someone else's work or ideas and passing them off as one's own.''\cite{oxed}  However, for this paper we will define it exclusivity as ``coping someone else's code to complete an assignment.''  We exclude copying ideas because it is incredible difficult to determine if an idea has been copied when implemented independently, the general culture of computer science is to iterate on others ideas (often without attribution), and because in the intro level, all the required ideas have been thought of by so many people determining an origin would be next to impossible.  Additionally, we exclude the possibility a student could ``cite'' their copied code to avoid plagiarism because the intro level is attempting to teach coding, not copying existing code.  This definition works well for the intro level, however in higher levels it becomes less useful.  We will use cheating and plagiarism synonymous in this paper.  We would also like to define some theoretical plagiarism ``vectors.''
\subsection*{\textbf{Vectors}}
\begin{itemize}
\item \textbf{P2P} Peer to Peer file sharing seems the most obvious vector.  This is where one student writes the code and shares it with a peer who then submits the copy.  In this instance one student is the author, and the other is the plagiarizer.  Although both students are at fault, the plagiarizer is more so than the author.  
	
	\item \textbf{Collaboration} Collaboration is a potential vector for individual assignments where two students work on the assignment together even though it is intended to be a solo exercise.  Collaboration is a particularly tricky vector as some level of collaboration is generally acceptable.  This vector is distinct from P2P because neither student can be pointed to as a distinct ``source''.
	
	\item \textbf{Theft}  While technically a subvector of P2P, Theft warrants its own definition.  Theft is any form of P2P plagiarism where the author does not know that it is happening.  For example, a student (the author works on a shared computer, and the plagiarizer finds the file when attempting the same assignment.  
	
	\item \textbf{Search}  Search engines, such as Google, are powerful tools, one of their powers is the ability to take an arbitrary string (Say, the title of a homework assignment for example) and get a useful result (like say, a github repo with the solution in 5 different programming languages).  We are defining any time a plagiarizer uses an internet search engine to find an existing solution to an assignment to be this vector of plagiarism.
	
	\item \textbf{Expert} Expert sources also exist.  These generally fall into n categorizes.
	\begin{itemize}
	\item Paywalled answer repositories:  This include sites like Chegg, or Coursehero.  On these (and similar sites), students can upload assignments to be done by expert programmers for a fee, or peruse other expert solutions hidden behind a paywall.  
	\item Freelance Coders: This includes any professional coder who will write code for student assignments.
	\item Unethical Tutors: This includes any person who purports to be a tutor, but completes assignments directly instead of guiding their student.
	\item Large Language Models:  This includes openly available generative models that can write code such as ChatGPT or Google's Bard.  These have gotten to the point where they can generate a bespoke solution to almost all intro level assignments if given an assignment as a prompt.
	
	
	\end{itemize} These are distinct from the Search vector for 2 reasons, first it is generally not free (or at least not freely accessible), and second the solution is tailored by the expert to the exact assignment given.  This renders any attempt to ``Scrape'' answers futile.
	This appears to have been the primary vector in the Motivating Case, the 10 identical submissions were all sourced from a Chegg ``Expert Answers'' Paywalled repo.
\end{itemize}
 It should be noted that all of these vectors can affect anything from single lines of code to entire assignments, and multiple vectors might be present in a single assignment.
	This makes it difficult to detect when they are happening, not only because it involves detecting plagiarism, but also because students are encouraged to use many of these vectors to aid their own work.  As such we will be focused more on substantial cases where a potential plagiarizer acquires a significant amount of an assignment from the vector.  For example, if a single line of code or a non required helper function is copied, probably not a problem; but if an entire required function is copied, that is a problem.

\n\section{\textbf{Current Approach}}
	There are many existing research projects attempting to detect Plagerism The general method of plagiarism can be summarized as follows.  Given a set $A$ of submissions find a set $P$ such that  $\forall p\in A,$ if $\exists q\in A$ such that $q\not=p$ and $D(p,q)$, then $p\in P$ and $q\in P$ for some plagiarism detection function $D$.  Defining $D$ efficiently can be tricky, however several researchers have done so.
	
		Stanford university developed a program called MOSS (Measure of Software Simularity) which is capable of giving each assignment pair a similarity score. This approach is robust against name changes, code reorganization, and white space changes by analyzing what and how a program does something, and not relying on the exact source code.[[How?]]
	
	
	There are many other efforts to define $D$ using a machine learning model.[[citations]]
	
	Unfortunately, regardless of the definition of $D$ used, all of these models have similar results when attempting to detect a given vector.
\subsection*{\textbf{Theoretical Per Vector Performance}}
	\subsubsection*{\textbf{P2P}}\hfill\break\indent
		P2P is handled fairly well by these models.  This vector is well suited for a source code analysis so long as $D$ is resistant to minor changes and all sources used are submitted by someone.  However, these models can not detect the author specifically.
	\subsubsection*{\textbf{Collaboration}}\hfill\break\indent
		Similarly to P2P, Collaboration is often detected by these models as the source code is quite similar.  $D$ however is harder to build for these as the ``minor'' changes can become more extreme with collaboration than with copying.  Additionally, when they do detect plagiarism, these models can not discriminate between P2P and Collaboration.
	\subsubsection*{\textbf{Theft}}\hfill\break\indent
		Theft is as easy to detect as P2P and these models detect it well, however they can not discriminate between the plagiarizer and the victim, nor can they discriminate between P2P and Theft.
	\subsubsection*{\textbf{Search}}\hfill\break\indent
		Search is where existing models begin to substantially fail.  No matter how sophisticated $D$ is, it requires all potential sources to be present to consider.  Considering the vast number of potential sources that can be found on the internet, detecting this vector is extremely difficult for $D$ and generally requires a human to put in the leg work to scrape together the list of potential sources.  Additionally, these searches generally take $O(s^2+se)$ Time (where $s$ is student submissions, and $e$ is external examples), for large values of $e$, this can be prohibitively long.
	\subsubsection*{\textbf{Expert} }\hfill\break\indent
	Expert is where existing models unambiguously fail the hardest.  A properly executed Expert vector is indistinguishable from a legitimate attempt for all possibly $D$.  As this content is Bespoke or behind a Paywall, it is genuinely impossible to seed the model with all relevant sources.  The only way these models can detect an Expert vector is if 2 students both submit the same Expert source by accident.
\subsection*{\textbf{General Shortcomings}}
In addition to the per vector performance, all of these models suffer from frequent false positives especially in the intro level.  Consider Hello World in java.  There is effectively 1 way to write hello world in java, you have your class, your main, and your print.  This is similar for nearly all other languages.  Since nearly all intro classes start with hello world, if the first assignment is checked with the above methods, it is likely that every student will be marked as plagiarized, even if they all attempted the assignment properly and individually.

\subsection*{\textbf{Notable Alternatives}}
There are other existing models that can potentially address these problems.  One research group used machine learning to detect stylistic variation in non-code reports. \cite{english}  This approach has potential, especially when applied across a student's entire code corpus[[double check usage of corpus]], and might be able to detect a change of Author compared to other assignments.  However, this vector is still weak to a serial plagiarist using a single source consistently.  Additionally our target is intro level students who may not yet have a defined ``style.''  We believe it would be reasonable to suppose an intro level student's style would drift significantly during their first few courses.  As such, we did not consider this approach.

There are also hyper-focused detection models that attempt to exclusively a detect single LLMs output.\cite{llm detect}  However, these are not particularly reliable at currently\cite{23 percent} and have a high false positive rate. \cite{and you fail}  At the time we started this research, LLMs were not a vector, and the hyper-focused and unreliability of these have prevented us from giving serious credence to this approach. 
\n\section{\textbf{Our Approach:} Single Source Plagiarism Detection}
For our approach, we decided that it is more useful to be able to determine if an individual file its self is plagiarized, rather than a collection.  To enable this, we created a unique IDE based on the Processing IDE.  When the IDE first launches it creates a persistent UUID on the machine that installed it.  This UUID acts as a machine or user ID, and is referred to as the \installID and is assumed to be constant throughout the class.  Additionally, whenever a project is created with the IDE, a second UUID is created unique to that project, we refer to this as the \projectID.  Both UUIDs are saved in a special hidden metadata comment in the program source file.

Whenever a file is opened, the IDE checks the \installID of the file and machine.  If they differ, the IDE notes the new \installID in the metadata in an ordered list called the \infectionStack.  

Additionally, when any part of the code is copied zero width spaces are interspersed with the normal letters to encode as much data as possible.  The exact amount of data varies based on the size of the copy, but it can include several copies of the \installID, \projectID, and \infectionStack.  When pasted into a project, the encoded data is compared to that of the project reviving the code, and any mismatch UUID's are added to the \infectionStack. This encoding survives being sent over most messaging software such as Discord, (but notably NOT GMail), and is generally resilient to partial pastes.  Any paste without encoded data is logged as originating outside the IDE.

In addition to copy tracking, the IDE also logs the time, location, and content of all edit events including copy, paste, cut, delete, and typing.  Any mismatched UUIDs are included in the log for the respective paste event.  This data is also included in the metadata.  Using this keylog, it is possible to reconstruct the entire coding process and trace all large code blocks.

Lastly, if the IDE loads a file without a metadata comment, it notes this as the first edit and logs the initial state of the file.

While it is generally enough to use a single file to detect Plagiarism, the UUIDs can be used to trace code authorship through multiple student submissions.

\subsection*{\textbf{Theoretical Per Vector Performance}}
This method was designed with our vectors in mind, and performs well on all of them.
\subsubsection*{\textbf{P2P}}\hfill\break\indent
		As with existing methods, P2P is easy to detect.  There is no easy way to submit someone else's unmodified file without the UUIDs being a clear red flag.  Any attempt to edit the file will mark it as infected.  Intro classes frequently have students add an ``Author'' comment to the top of their files.  This would be need to be edited by anyone attempting to cheat, and the original comment is preserved in the log. Finally any code shared via IMing will be flagged at they very least, and generally, traced.   If a student copies from a previous assignment, it will be traceable which, and who authored that assignment.
		
		Even simple files such as Hello World would be detected, as the detection does not rely on code comparison.
	\subsubsection*{\textbf{Collaboration}}\hfill\break\indent
		Collaborating students will have similar code with overlapping timestamps at each edit.  Additionally, collaborating students tend to send code back and forth, meaning both side's \infectionStack will be populated with each other's UUIDs.  This back and forth makes it easy to distinguish from P2P.
	\subsubsection*{\textbf{Theft}}\hfill\break\indent
	Theft is detected in the same way as P2P.  Unfortunately it can be tricky to discriminate it from P2P, but it is now theoretically possible with meta knowledge of the computer setup (i.e. class computers will have a known \installID) and the \infectionStack combined with timestamped log make it possible to isolate 1 copy as the origin IF it is edited on the same or a different computer.  
	\subsubsection*{\textbf{Search}}\hfill\break\indent
		All Search vectors are soundly defeated.  No matter how a file is found, once it is loaded by the IDE, it will be marked as external. If it is submitted without being opened by the IDE, it will lack a meta comment.  If the source code is copied into an existing project, it will be marked as external.  The only viable source that this does not detect is physically typing out the code from a reference.  This is easily detected by analyzing the timestamp data.  Organically written code has a lot of back and forth.  For example, Java's Hello World tends to be typed as 
		[[XXX Figure out a code env for this]]
		class HW{
		
		}
		[UP ARROW]
		public static void main(String[] args){
		
		}
		[UP ARROW]
		System.out.println("Hello World");
		[RUN]
		
		Where as a plagiarist copying it by typing would likely write it top to bottom without backtracking significantly.
	\subsubsection*{\textbf{Expert} }\hfill\break\indent
	Expert sources are detected in the same way as Search.  The only way an Expert can avoid detection as an external source is by installing the IDE and using that, in which case THEY will have a different \installID from the student.  This may require multiple assignments to detect, but is likely reliable over a full class unless the student finds 1 consistent Expert, or the Expert uses the student's own computer.
\subsection*{\textbf{Evasion}}
There are notable ways to avoid detection by this method.  In general we have dismissed them as ``being more work than just DOING the assignment.''  The meta comment is a plain text java comment appended to the file.  Any text editor OTHER than our IDE will show this comment and allow it to be edited.  Convincingly faking the edit log would be incredible difficult, and deleting the entire string is easily detected.

A more viable method would be to share the \installID file with an Expert.


A corrupted meta comment is another possibility.  The comment could be tampered with in such a way that it is incomplete.  Sadly, this occasionally legitimately DOES happen so this is a legitimate concern.

We have dismissed these weaknesses reasoning that if an intro level student discovers AND is able to exploit them, they likely know enough to not NEED to plagiarize in the intro level.  And as the program is currently not well known, there will be no external assistance in doing so.

A potential bypass is to complete the assignment via any means that doesnt add to the \infectionStack, and then copy and paste the file content into another copy of the IDE, this then gets logged as an internal paste events between assignments and all keystroke logging is lost.  This method we dismiss as there is little reason to do it (other than IDE bugs) and it is easily detectable as an event of interest.

\n\section*{\textbf{Case Study}}
During the Spring 2023 ICS 111 and 211 Intro to Java classes, we offered an extra credit assignment to students. [[XXX what do I have to do to reference the ethics board here?]]
Students were instructed to use the IDE and ATTEMPT to plagiarize the assignment.  We worked with the professors of the classes to give each class a project that would be slightly too difficult to the average person in the class to encourage plagiarism, but not so difficult that NO students in the class would be able to complete it on their own.  The 111 class was given a project to make 3 Polymorphic Shapes that move around the screen and bounce off the edges.  The 211 class was given the task to create any function defined fractal (such as a Mandelbrot fractal).  ChatGPT was able to generate solutions to both without assistance, and a solution for each was seeded on to an assortment of websites.  Assignments were collected completely anonymously by distributing and collecting identical thumbdrives containing the IDE which was set to save to the drive its self.  Assignment credit was given for turning in a thumbdrive without checking its content.  2 students in 211 caught this loophole and did in-fact turn in completely blank thumbdrives.

Students were also asked to self report the method they used to complete the assignment and include the report on the thumbdrive.

As the 111 and 211 classes are separate assignments and cohorts, we will treat them as 2 concurrent Case Studies. 

As part of each Case Study, the meta comment was stripped out of the files, and they were given to Stanford MOSS.

For more detailed code analysis, see Appendix A [[XXXX REF THIS]].
\subsection{\textbf{ICS 111}}
We received 5 submissions from 111, these have each been arbitrary given a single letter designation between A and F.  Of these 5, 3 successfully completed the assignment by some means, and 2 did not.  Student A copy and pasted from ChatGPT, but prompted it wrong, and did a similar but wrong assignment.  

Student D appears to have gone insane, and the code they copied is a badly mangled AWT assignment and does not compile without significant modification.

Of the Correct assignments Student B completed the assignment Legitimately and it is quite obvious in the meta strings.  There are multiple well organized files each organically coded with no large external pastes or irregularly linear typing sections.

Student C used the copy paste exploit mentioned previously and did not leave a comment.  As such, we can not determine if their code is legitimate or not.

Student E pasted a large section of code from a different project with a matching \installID and then spent considerable time debugging it.  We believe this to be a legitimate completion.

Student F found and submitted the one of the seeded assignment file.  It is logged as a large foreign paste.

\n\subsubsection*{\textbf{MOSS}}\hfill\break\indent
Despite having 4 files containing plagiarized code, Moss only identified a small connection between Student C and E.  This connection is entirely composed of expected driver code. 
\n\subsubsection*{\textbf{Observed Vectors}}\hfill\break\indent
Between all the submissions meta comments we were able to identify that Student A and F Expert or Search; Student B and E completed the assignment on their own; Student D used Search; and only student C is of indeterminate origin, but has a clean \infectionStack.

[[Add Table]]
\subsection*{\textbf{ICS 211}}

\n\subsubsection*{\textbf{Observed Vectors}}\hfill\break\indent


\n\section{\textbf{Conclusion}}
a
\bibliography{references}{}
 \bibliographystyle{plain}

\section{\textbf{Apendix A: Examples}}
\end{document}

